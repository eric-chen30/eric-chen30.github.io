<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://csstar.top</id>
    <title>CsStar • Posts by &#34;机器学习&#34; tag</title>
    <link href="http://csstar.top" />
    <updated>2020-09-15T06:02:26.000Z</updated>
    <category term="随便写点儿星季" />
    <category term="小的知识点" />
    <category term="计算机组成原理" />
    <category term="Docker" />
    <category term="机器学习" />
    <category term="单例模式" />
    <category term="Pandas" />
    <entry>
        <id>http://csstar.top/2020/09/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E5%90%8E%E4%B9%A0%E9%A2%98/</id>
        <title>机器学习之模型评估</title>
        <link rel="alternate" href="http://csstar.top/2020/09/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E5%90%8E%E4%B9%A0%E9%A2%98/"/>
        <content type="html">&lt;h1 id=&#34;假设空间与版本空间&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#假设空间与版本空间&#34;&gt;#&lt;/a&gt; 假设空间与版本空间&lt;/h1&gt;
&lt;h2 id=&#34;概念&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#概念&#34;&gt;#&lt;/a&gt; 概念&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;假设空间：所有可能取到的值组成的样本。&lt;/p&gt;
&lt;p&gt;版本空间：与给定正样本一致和与负样本不一致的假设子集集合。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;计算&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#计算&#34;&gt;#&lt;/a&gt; 计算&lt;/h2&gt;
&lt;p&gt;假设西瓜由青绿、乌黑、浅白三种颜色，同理，根蒂和敲声也由三种属性完全定义。求假设空间：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;假设空间大小：&lt;/p&gt;
&lt;p&gt;（3+1）✖（3+1）✖（3+1）+1 = 65&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;（3+1）：就拿颜色来说，可能任意一种颜色都可以，其他属性也是一样。&lt;/p&gt;
&lt;p&gt;（+1）：世界上没有好瓜&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;版本空间：根据给定的样本，从假设空间中&lt;strong&gt;剔除&lt;/strong&gt;与正样本不一致，与负样本一致的假设。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;模型评估与选择&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#模型评估与选择&#34;&gt;#&lt;/a&gt; 模型评估与选择&lt;/h1&gt;
&lt;h2 id=&#34;过拟合与欠拟合&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#过拟合与欠拟合&#34;&gt;#&lt;/a&gt; 过拟合与欠拟合&lt;/h2&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;过拟合：对训练样本的特征训练的太好了，导致泛化能力下降，对新样本的预测很差的现象。&lt;/li&gt;
&lt;li&gt;欠拟合：训练样本的特征不够，泛化能力太高了。比如模型可能训练出绿色是叶子的结论，那么对于一棵树，模型也将树判断为叶子。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;评估方法&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#评估方法&#34;&gt;#&lt;/a&gt; 评估方法&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;通常用测试误差来近似模型的泛化误差。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;留出法hold-out&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#留出法hold-out&#34;&gt;#&lt;/a&gt; 留出法（hold out）&lt;/h3&gt;
&lt;p&gt;留出法直接将数据集划分为两个互斥的部分，其中一部分用来做训练集，另一部分用来做测试集。通常训练集与测试集的比例为 7：3。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;注意：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;尽可能保持数据分布的一致性。通常采用 “分层采样” 的方法。&lt;/li&gt;
&lt;li&gt;采用若干次随机划分避免单次流出法的不稳定。&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;交叉验证法cross-validation&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#交叉验证法cross-validation&#34;&gt;#&lt;/a&gt; 交叉验证法（cross validation)&lt;/h3&gt;
&lt;p&gt;交叉验证法先将数据集划分为 k 个大小相似的互斥子集，每次采用 k-1 个子集作为训练集，剩下的一个子集作为测试集。进行 k 次训练和测试，最终返回 k 个测试结果的均值。又称为 “k 折交叉验证”&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;通常重复 p 次不同的划分，最终结果是 p 次 k 折交叉验证结果的均值。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;留一法leave-one-outloo&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#留一法leave-one-outloo&#34;&gt;#&lt;/a&gt; 留一法（leave-one-out，LOO）&lt;/h3&gt;
&lt;p&gt;留一法是交叉验证法的一种特殊情况，k = 样本数，即将样本分成一份一个样本，每次测试集只有一个样本。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;该方法的确定，计算的开销比较大。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;自助法bootstrapping&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#自助法bootstrapping&#34;&gt;#&lt;/a&gt; 自助法（bootstrapping）&lt;/h3&gt;
&lt;p&gt;自助法以自助采样为基础（有放回采样）。每次随机从数据集中挑选一个样本，放入另一个数据集合，然后将样本放回原数据中，重复操作 m 次，因此得到了一个含 m 个样本的数据集。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;样本在 m 次采样中始终不被采到的概率（1-1\m）的 m 次方，当 m 足够大时，上述式子的极限约为 0.368。说明有 0.368 的样本不会出现在新的数据集中，新数据集中含有更多的数据，我们可以用来当作训练集，剩下的做测试集。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;优点：在数据集较小、难以有效划分训练和测试集的时候很有用。&lt;/li&gt;
&lt;li&gt;改变了初始数据集的分布，会引入估计偏差。&lt;/li&gt;
&lt;/ul&gt;
</content>
        <category term="机器学习" />
        <updated>2020-09-15T06:02:26.000Z</updated>
    </entry>
</feed>
